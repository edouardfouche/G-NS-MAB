Our approaches

DONE:
MP_AW_UCB
MP_AW_KL_UCB
MP_AW_TS
MP_AWR_TS

Elimination? (TODO) TODO: What is the relationship with MP_AWR_TS?

Consider to use ADWIN1/2 maybe? (No)

Active methods: TODO

Adapt-EvE (Combines UCB with PH test, no regret bound)
-- Hartland et al. (2007).  Change Point Detection and Meta-Bandits for Online Learning in Dynamic Environments

Change-Point Thompson Sampling (CTS) (no regret bound)
-- Mellor, J. C. and Shapiro, J. (2013).  Thompson sampling in switching environments with bayesian online change detection

CUSUM-UCB and PH-UCB (resetting, forced exploration, regret bound)
-- Liu, F., Lee, J., and Shroff, N. B. (2018). A change-detection based framework for piecewise-stationary multi-armed bandit problem.

M-UCB (UCB + adaptive-window based resetting method, regret bound)
-- Cao, Y., Wen, Z., Kveton, B., and Xie, Y. (2019). Nearly optimal adaptive procedure with change detection for piecewise-stationary bandit

GLR-klUCB (forced exploration, regret bound)
-- Besson, L. and Kaufmann, E. (2019).  The generalized likelihood ratio test meets klucb: an improved algorithm for piece-wise non-stationary bandits
DONE

Exp3.R (Exp3 + change detection, regret bound)
-- Allesiardo, R. and Féraud, R. (2015). EXP3 with drift detection for the switching bandit problem

Passive Methods: DONE

D-UCB (Discounted UCB, regret bound)
-- Kocsis, L. and Szepesvári, C. (2006). Discounted ucb. In2nd PASCAL Challenges Workshop, pages 784–791
DONE -> See bandits > nonstationary > MP_D_UCB

SW-UCB (Sliding Window UCB, regret bound)
-- Garivier, A. and Moulines, E. (2008). On Upper-Confidence Bound Policies for Non-Stationary Bandit Problems.
DONE -> See bandits > nonstationary > MP_SW_UCB

SW-TS (Sliding Window Thompson Sampling, regret bound)
-- Trovò, F., Restelli, M., and Gatti, N. (2020). Sliding-window thompson sampling for non-stationary settings
DONE -> See bandits > nonstationary > MP_SW_TS

SW-UCB# (regret bounds)
-- Wei, L. and Srivastava, V. (2018).  On abruptly-changing and slowly-varying multiarmed bandit problems.
DONE -> See bandits > nonstationary > MP_SW_UCB_SHARP_A
DONE -> See bandits > nonstationary > MP_SW_UCB_SHARP_G
We disregard LM-DSEE therein because SW-UCB# is better

Rexp3 (regret bound, limited variation)
-- Besbes, O., Gur, Y., and Zeevi, A. (2014).  Optimal exploration-exploitation in a multi-armed-bandit problem with non-stationary rewards.
DONE -> See bandits > adversarial > MP_RExp3

Other competitors: (DONE)

MP_D_TS + variant MP_D_OTS
MP_E_Greedy


Data:

- Yahoo! Dataset:
\url{https://webscope.sandbox.yahoo.com/catalog.php?datatype=\%20r&did=49&guccounter=1}

- zozo: \url{https://github.com/st-tech/zr-obp}

- Bioliq